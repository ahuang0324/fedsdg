#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FedSDG åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. LoRALayer åœ¨ FedSDG æ¨¡å¼ä¸‹çš„å‚æ•°åˆå§‹åŒ–
2. å‰å‘ä¼ æ’­çš„åŒè·¯è®¡ç®—
3. get_lora_state_dict æ­£ç¡®è¿‡æ»¤ç§æœ‰å‚æ•°
4. é€šä¿¡é‡ç»Ÿè®¡ä¸ FedLoRA ä¸€è‡´
5. å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from models import LoRALayer, get_lora_state_dict, ViT, inject_lora


def test_lora_layer_fedsdg():
    """æµ‹è¯• LoRALayer åœ¨ FedSDG æ¨¡å¼ä¸‹çš„åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­"""
    print("\n" + "="*70)
    print("æµ‹è¯• 1: LoRALayer FedSDG æ¨¡å¼åˆå§‹åŒ–")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
    original_layer = nn.Linear(128, 64)
    
    # æµ‹è¯•æ ‡å‡† LoRA æ¨¡å¼
    print("\n[æ ‡å‡† LoRA æ¨¡å¼]")
    lora_standard = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=False)
    
    # æ£€æŸ¥æ ‡å‡† LoRA çš„å‚æ•°
    standard_params = dict(lora_standard.named_parameters())
    print(f"  å‚æ•°åˆ—è¡¨: {list(standard_params.keys())}")
    assert 'lora_A' in standard_params, "æ ‡å‡† LoRA åº”è¯¥æœ‰ lora_A"
    assert 'lora_B' in standard_params, "æ ‡å‡† LoRA åº”è¯¥æœ‰ lora_B"
    assert 'lora_A_private' not in standard_params, "æ ‡å‡† LoRA ä¸åº”è¯¥æœ‰ lora_A_private"
    assert 'lambda_k_logit' not in standard_params, "æ ‡å‡† LoRA ä¸åº”è¯¥æœ‰ lambda_k_logit"
    print("  âœ“ æ ‡å‡† LoRA å‚æ•°æ£€æŸ¥é€šè¿‡")
    
    # æµ‹è¯• FedSDG æ¨¡å¼
    print("\n[FedSDG æ¨¡å¼]")
    lora_fedsdg = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=True)
    
    # æ£€æŸ¥ FedSDG çš„å‚æ•°
    fedsdg_params = dict(lora_fedsdg.named_parameters())
    print(f"  å‚æ•°åˆ—è¡¨: {list(fedsdg_params.keys())}")
    assert 'lora_A' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_A (å…¨å±€åˆ†æ”¯)"
    assert 'lora_B' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_B (å…¨å±€åˆ†æ”¯)"
    assert 'lora_A_private' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_A_private (ç§æœ‰åˆ†æ”¯)"
    assert 'lora_B_private' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_B_private (ç§æœ‰åˆ†æ”¯)"
    assert 'lambda_k_logit' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lambda_k_logit (é—¨æ§å‚æ•°)"
    print("  âœ“ FedSDG å‚æ•°æ£€æŸ¥é€šè¿‡")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n[å‰å‘ä¼ æ’­æµ‹è¯•]")
    x = torch.randn(4, 128)  # batch_size=4, in_features=128
    
    # æ ‡å‡† LoRA å‰å‘ä¼ æ’­
    output_standard = lora_standard(x)
    print(f"  æ ‡å‡† LoRA è¾“å‡ºå½¢çŠ¶: {output_standard.shape}")
    assert output_standard.shape == (4, 64), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (4, 64)"
    
    # FedSDG å‰å‘ä¼ æ’­
    output_fedsdg = lora_fedsdg(x)
    print(f"  FedSDG è¾“å‡ºå½¢çŠ¶: {output_fedsdg.shape}")
    assert output_fedsdg.shape == (4, 64), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (4, 64)"
    
    # æ£€æŸ¥ lambda_k çš„èŒƒå›´
    lambda_k = torch.sigmoid(lora_fedsdg.lambda_k_logit)
    print(f"  é—¨æ§å‚æ•° lambda_k: {lambda_k.item():.4f}")
    assert 0 <= lambda_k.item() <= 1, "lambda_k åº”è¯¥åœ¨ [0, 1] èŒƒå›´å†…"
    print("  âœ“ å‰å‘ä¼ æ’­æ£€æŸ¥é€šè¿‡")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 1 é€šè¿‡ï¼šLoRALayer FedSDG æ¨¡å¼å·¥ä½œæ­£å¸¸")
    print("="*70)


def test_get_lora_state_dict_filtering():
    """æµ‹è¯• get_lora_state_dict æ­£ç¡®è¿‡æ»¤ç§æœ‰å‚æ•°"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: get_lora_state_dict ç§æœ‰å‚æ•°è¿‡æ»¤")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ ViT æ¨¡å‹å¹¶æ³¨å…¥ FedSDG
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,  # ä½¿ç”¨ 2 å±‚ä»¥åŠ å¿«æµ‹è¯•
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # è·å–å®Œæ•´çš„ state_dict
    full_state = model.state_dict()
    print(f"\n[å®Œæ•´ state_dict]")
    print(f"  æ€»å‚æ•°æ•°é‡: {len(full_state)}")
    
    # ç»Ÿè®¡ç§æœ‰å‚æ•°
    private_params = [k for k in full_state.keys() if '_private' in k or 'lambda_k' in k]
    print(f"  ç§æœ‰å‚æ•°æ•°é‡: {len(private_params)}")
    print(f"  ç§æœ‰å‚æ•°ç¤ºä¾‹: {private_params[:3] if len(private_params) > 0 else 'None'}")
    
    # è·å– LoRA state_dictï¼ˆåº”è¯¥è¿‡æ»¤æ‰ç§æœ‰å‚æ•°ï¼‰
    lora_state = get_lora_state_dict(model)
    print(f"\n[LoRA state_dict (ç”¨äºé€šä¿¡)]")
    print(f"  å‚æ•°æ•°é‡: {len(lora_state)}")
    
    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®è¿‡æ»¤
    for key in lora_state.keys():
        assert '_private' not in key, f"ä¸åº”è¯¥åŒ…å«ç§æœ‰å‚æ•°: {key}"
        assert 'lambda_k' not in key, f"ä¸åº”è¯¥åŒ…å«é—¨æ§å‚æ•°: {key}"
    
    print("  âœ“ æ‰€æœ‰ç§æœ‰å‚æ•°å·²è¢«æ­£ç¡®è¿‡æ»¤")
    
    # æ£€æŸ¥å…¨å±€å‚æ•°æ˜¯å¦å­˜åœ¨
    global_params = [k for k in lora_state.keys() if 'lora_A' in k or 'lora_B' in k]
    print(f"  å…¨å±€ LoRA å‚æ•°æ•°é‡: {len(global_params)}")
    assert len(global_params) > 0, "åº”è¯¥åŒ…å«å…¨å±€ LoRA å‚æ•°"
    print("  âœ“ å…¨å±€å‚æ•°æ­£ç¡®ä¿ç•™")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 2 é€šè¿‡ï¼šç§æœ‰å‚æ•°è¿‡æ»¤åŠŸèƒ½æ­£å¸¸")
    print("="*70)


def test_communication_volume():
    """æµ‹è¯• FedSDG çš„é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: FedSDG ä¸ FedLoRA é€šä¿¡é‡å¯¹æ¯”")
    print("="*70)
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„ ViT æ¨¡å‹
    model_fedlora = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    model_fedsdg = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ LoRA
    model_fedlora = inject_lora(model_fedlora, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=False)
    model_fedsdg = inject_lora(model_fedsdg, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # è·å–é€šä¿¡å‚æ•°
    lora_state_fedlora = get_lora_state_dict(model_fedlora)
    lora_state_fedsdg = get_lora_state_dict(model_fedsdg)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    params_fedlora = sum(p.numel() for p in lora_state_fedlora.values())
    params_fedsdg = sum(p.numel() for p in lora_state_fedsdg.values())
    
    print(f"\n[é€šä¿¡å‚æ•°ç»Ÿè®¡]")
    print(f"  FedLoRA é€šä¿¡å‚æ•°: {params_fedlora:,}")
    print(f"  FedSDG é€šä¿¡å‚æ•°:  {params_fedsdg:,}")
    print(f"  å·®å¼‚: {abs(params_fedlora - params_fedsdg):,}")
    
    # è®¡ç®—é€šä¿¡é‡ï¼ˆMBï¼‰
    bytes_per_param = 4  # float32
    comm_mb_fedlora = (params_fedlora * bytes_per_param) / (1024 ** 2)
    comm_mb_fedsdg = (params_fedsdg * bytes_per_param) / (1024 ** 2)
    
    print(f"\n[é€šä¿¡é‡ (MB)]")
    print(f"  FedLoRA: {comm_mb_fedlora:.4f} MB")
    print(f"  FedSDG:  {comm_mb_fedsdg:.4f} MB")
    
    # éªŒè¯é€šä¿¡é‡ä¸€è‡´
    assert params_fedlora == params_fedsdg, "FedSDG çš„é€šä¿¡é‡åº”è¯¥ä¸ FedLoRA å®Œå…¨ä¸€è‡´"
    print("\n  âœ“ é€šä¿¡é‡å®Œå…¨ä¸€è‡´")
    
    # ç»Ÿè®¡ FedSDG çš„æ€»å‚æ•°ï¼ˆåŒ…æ‹¬ç§æœ‰å‚æ•°ï¼‰
    total_params_fedsdg = sum(p.numel() for p in model_fedsdg.parameters() if p.requires_grad)
    private_params = total_params_fedsdg - params_fedsdg
    
    print(f"\n[FedSDG å‚æ•°åˆ†å¸ƒ]")
    print(f"  æ€»å¯è®­ç»ƒå‚æ•°: {total_params_fedsdg:,}")
    print(f"  é€šä¿¡å‚æ•° (å…¨å±€): {params_fedsdg:,}")
    print(f"  ç§æœ‰å‚æ•° (æœ¬åœ°): {private_params:,}")
    print(f"  ç§æœ‰å‚æ•°å æ¯”: {100 * private_params / total_params_fedsdg:.2f}%")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 3 é€šè¿‡ï¼šFedSDG é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´")
    print("="*70)


def test_private_state_management():
    """æµ‹è¯•å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€çš„ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "="*70)
    print("æµ‹è¯• 4: å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯è®­ç»ƒï¼šä¿å­˜ç§æœ‰çŠ¶æ€
    print("\n[æ¨¡æ‹Ÿå®¢æˆ·ç«¯ 0 è®­ç»ƒ]")
    
    # æå–ç§æœ‰å‚æ•°
    private_state = {}
    for name, param in model.named_parameters():
        if '_private' in name or 'lambda_k' in name:
            private_state[name] = param.data.clone()
    
    print(f"  æå–ç§æœ‰å‚æ•°æ•°é‡: {len(private_state)}")
    print(f"  ç§æœ‰å‚æ•°ç¤ºä¾‹: {list(private_state.keys())[:3]}")
    
    # ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒï¼‰
    for param in model.parameters():
        if param.requires_grad:
            param.data += torch.randn_like(param.data) * 0.01
    
    # ä¿å­˜ä¿®æ”¹åçš„ç§æœ‰å‚æ•°
    private_state_after = {}
    for name, param in model.named_parameters():
        if '_private' in name or 'lambda_k' in name:
            private_state_after[name] = param.data.clone()
    
    # éªŒè¯ç§æœ‰å‚æ•°å·²æ”¹å˜
    changed = False
    for key in private_state.keys():
        if not torch.allclose(private_state[key], private_state_after[key]):
            changed = True
            break
    
    assert changed, "ç§æœ‰å‚æ•°åº”è¯¥åœ¨è®­ç»ƒåå‘ç”Ÿå˜åŒ–"
    print("  âœ“ ç§æœ‰å‚æ•°åœ¨è®­ç»ƒåå·²æ›´æ–°")
    
    # æ¨¡æ‹ŸåŠ è½½ç§æœ‰çŠ¶æ€åˆ°æ–°æ¨¡å‹
    print("\n[æ¨¡æ‹Ÿä¸‹ä¸€è½®ï¼šåŠ è½½ç§æœ‰çŠ¶æ€]")
    
    # åˆ›å»ºæ–°çš„å…¨å±€æ¨¡å‹ï¼ˆæ¨¡æ‹ŸæœåŠ¡å™¨èšåˆåçš„æ¨¡å‹ï¼‰
    model_new = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    model_new = inject_lora(model_new, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # åŠ è½½ç§æœ‰çŠ¶æ€
    current_state = model_new.state_dict()
    for param_name, param_value in private_state_after.items():
        if param_name in current_state:
            current_state[param_name] = param_value.clone()
    model_new.load_state_dict(current_state)
    
    # éªŒè¯ç§æœ‰å‚æ•°å·²æ­£ç¡®åŠ è½½
    for name, param in model_new.named_parameters():
        if name in private_state_after:
            assert torch.allclose(param.data, private_state_after[name]), f"å‚æ•° {name} åŠ è½½å¤±è´¥"
    
    print("  âœ“ ç§æœ‰çŠ¶æ€æˆåŠŸåŠ è½½åˆ°æ–°æ¨¡å‹")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 4 é€šè¿‡ï¼šç§æœ‰çŠ¶æ€ç®¡ç†åŠŸèƒ½æ­£å¸¸")
    print("="*70)


def test_forward_backward():
    """æµ‹è¯• FedSDG çš„å‰å‘å’Œåå‘ä¼ æ’­"""
    print("\n" + "="*70)
    print("æµ‹è¯• 5: FedSDG å‰å‘å’Œåå‘ä¼ æ’­")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # åˆ›å»ºè¾“å…¥å’Œæ ‡ç­¾
    x = torch.randn(2, 3, 32, 32)
    y = torch.randint(0, 10, (2,))
    
    # å‰å‘ä¼ æ’­
    print("\n[å‰å‘ä¼ æ’­]")
    output = model(x)
    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (2, 10), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (2, 10)"
    print("  âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
    
    # è®¡ç®—æŸå¤±
    criterion = nn.CrossEntropyLoss()
    loss = criterion(output, y)
    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    print("\n[åå‘ä¼ æ’­]")
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    grad_count = 0
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            grad_count += 1
    
    print(f"  æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {grad_count}")
    assert grad_count > 0, "åº”è¯¥æœ‰å‚æ•°å…·æœ‰æ¢¯åº¦"
    print("  âœ“ åå‘ä¼ æ’­æˆåŠŸ")
    
    # æ£€æŸ¥ç§æœ‰å‚æ•°ä¹Ÿæœ‰æ¢¯åº¦
    private_grad_count = 0
    for name, param in model.named_parameters():
        if ('_private' in name or 'lambda_k' in name) and param.grad is not None:
            private_grad_count += 1
    
    print(f"  ç§æœ‰å‚æ•°æœ‰æ¢¯åº¦çš„æ•°é‡: {private_grad_count}")
    assert private_grad_count > 0, "ç§æœ‰å‚æ•°åº”è¯¥æœ‰æ¢¯åº¦"
    print("  âœ“ ç§æœ‰å‚æ•°å¯ä»¥æ­£å¸¸è®­ç»ƒ")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 5 é€šè¿‡ï¼šå‰å‘å’Œåå‘ä¼ æ’­æ­£å¸¸")
    print("="*70)


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("FedSDG åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("="*70)
    
    try:
        test_lora_layer_fedsdg()
        test_get_lora_state_dict_filtering()
        test_communication_volume()
        test_private_state_management()
        test_forward_backward()
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FedSDG å®ç°æ­£ç¡®ï¼")
        print("="*70)
        print("\næ€»ç»“ï¼š")
        print("  âœ“ LoRALayer åŒè·¯æ¶æ„å·¥ä½œæ­£å¸¸")
        print("  âœ“ ç§æœ‰å‚æ•°è¿‡æ»¤åŠŸèƒ½æ­£ç¡®")
        print("  âœ“ é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´ï¼ˆ0.2MBï¼‰")
        print("  âœ“ å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†æ­£å¸¸")
        print("  âœ“ å‰å‘å’Œåå‘ä¼ æ’­æ­£å¸¸")
        print("\nFedSDG å·²å‡†å¤‡å¥½ç”¨äºè”é‚¦å­¦ä¹ è®­ç»ƒï¼")
        print("="*70 + "\n")
        
        return True
        
    except AssertionError as e:
        print("\n" + "="*70)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("="*70 + "\n")
        return False
    except Exception as e:
        print("\n" + "="*70)
        print(f"âŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        print("="*70 + "\n")
        return False


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
